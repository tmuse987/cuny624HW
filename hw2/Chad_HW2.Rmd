---
title: 'DATA 624 Spring 2019: Homework-2'
author: "Ahmed Sajjad, Harpreet Shoker, Jagruti Solao, Chad Smith, Todd Weigel"
date: "April 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE}
library(mice)
library(caret)
library(ggplot2)
library(corrplot)
library(olsrr)
library(nnet)
library(kernlab)
library(rminer)
```

##### <span style="color:blue"><b><u>KJ# 6.3</u></b></span>
<div id="KJ_6.3.PNG"><img src="KJ_6.3.PNG" alt="KJ_6.3.PNG"></div>
#### a) Start R and use the following commands to load the data. The matrix `processPredictors` contains 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. `yield` contains the percent yield for each run.

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)

processPredictors <- ChemicalManufacturingProcess
```

#### b) A small percentage of cells in the redictor set contain missing values. Use an imputation function to fill in these missing values

The summary for this dataset shows that many columns have 5 or less missing values. The column `ManufacturingProcess03` had the most missing values with 15.

```{r}
summary(processPredictors)
```

Using the MICE package, the 'predictive mean matching' method was used to impute missing values on the dataset.  
It ran through 50 iterations.

```{r message=FALSE, include=FALSE}
imputedData <- mice(processPredictors, m=1, maxit=50, method = 'pmm')
completeData <- complete(imputedData,1)
```

Summary of the completed dataset shows all missing values have been imputed.

```{r}
summary(completeData)
```

#### c) Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter.  What is the optimal value of the performance metric?

```{r}
#Smallest negative value is -1.8 so we'll add 2 to the column to make it positive for log tranformations
completeData$ManufacturingProcess21 <- completeData$ManufacturingProcess21 + 2

## 75% of the sample size
smp_size <- floor(0.75 * nrow(completeData))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(completeData)), size = smp_size)

train <- completeData[train_ind, ]
test <- completeData[-train_ind, ]
```

Pre-Process and transform the Data

```{r}


y <- train$Yield

train.trim <- subset(train, select = -c(Yield))

#The preProcess function returns a list of elements, such as vectors of standard deviations
# and vector of means since scale and center were reqeuested.
prepro_set <- preProcess(train.trim, method = c('scale', 'center'))

print(prepro_set)

#The predict function uses the list of elements to produce a dataframe of pre-processed variables
transformed_train <- predict(prepro_set, train.trim)
```

Building the Model using all predictor variables.

```{r}
transformed_train$Yield <- y

model1 <- lm(Yield ~., data = transformed_train)

summary(model1)
```

The R-Squared value is at 0.8072 which tells us that the model is a decent fit for the data.  The Adjusted R-Squared value decreases to 0.6632 which also tells us that many of predictors do not improve the model as much as they should. None of the predictors show to be significant to the model since most have high p-values. 

*Tuning the Model*

Using backward stepwise regression. This takes all the variables and removes them based on their p-value. The variables left have the most significant impact on the model. 

```{r}
model.tune <- lm(Yield ~., data = transformed_train)
k <- ols_step_backward_p(model.tune)


```

```{r}
x <- k$removed
print(x)
```


The stepwise model removed all variables with a low p-value leaving. A total of 21 variables were removed, the majority of them ManufacturingProcess variables. The R-Squared value was almost the exact same at 0.796 with an Adjusted R-Square value at 0.722.  With less variables without losing accuracy, I would use this as the primary model. 
#### d) Predict the response for the test set.  What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

```{r}
y_test <- test$Yield
test.tune <- subset(test, select = -c(Yield))

#Drop columns that were removed in the previous backward stepwise model
test.tune <- test.tune[, !(names(test.tune) %in% x)]

#The preProcess function returns a list of elements, such as vectors of standard deviations
# and vector of means since scale and center were reqeuested.
test_set <- preProcess(test.tune, method = c('scale', 'center'))

print(test_set)

#The predict function uses the list of elements to produce a dataframe of pre-processed variables
transformed_test <- predict(test_set, test.tune)

pred <- predict(k$model, transformed_test)

```

```{r}
qplot(seq_along(transformed_train$Yield), transformed_train$Yield, ylab = 'Yield', xlab = 'Index', main = 'Yield Actual Data', geom = 'point')
```

```{r}
qplot(seq_along(pred), pred, ylab = 'Yield', xlab = 'Index', main = 'Yield Predicted Data', geom = 'point')
```

RSME for predicted values
```{r}
RMSE(pred, y_test)
```

The performance metric was worse for the predicted data than the training data. Using the RSME metric, the training data had a 0.945.  The testing data showed a 4.11 with the plots clearly showing how big the variance is between actual and predicted data. 

#### e) Which predictors are most important in the model you have trained?  Do either the biological or process predictors dominate the list?

The original dataset had 12 biological predictors and 45 process predictors. The backwards stepwise regression function dropped 3 biological and 15 process predictors, or 25% and 33% respectively.  Process predictors dominate the list but the stepwise model calculated the a higher percentage of those were insignificant to the model.

#### f) Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?

```{r}
summary(k$model)
```

Top predictor variables in this model is BiologicalMaterial03, BiologicalMaterial06, BiologicalMaterial09, ManufacturingProcess29, ManufacturingProcess32, and ManufacturingProcess37 based on their p-values.

These predictors were slightly significant with the model of all variables though they became very significant when the stepwise function dropped insignificant variables.

```{r}
top <- subset(test, select = c(Yield, BiologicalMaterial03, BiologicalMaterial06, BiologicalMaterial09,
                               ManufacturingProcess29, ManufacturingProcess32, ManufacturingProcess37))

m <- cor(top)
corrplot(m, method = 'circle')
```
From the correlation plot we see the BiologicalMaterial03 and 06 are highly correlated and may be redundant in the model.  For the Manufacturing Process predictors, #32 seems to be the most correlated with the Yield. For future runs, more attention could be made to this process to help improve the yield. 

##### <span style="color:blue"><b><u>KJ# 7.5</u></b></span>
<div id="KJ_7.5.PNG"><img src="KJ_7.5.PNG" alt="KJ_7.5.PNG"></div>
#### a) Which nonlinear regression model gives the optimal resampling and test set performance.


```{r}
#Get testing data and scale and center it

y_test <- test$Yield
test.tune <- subset(test, select = -c(Yield))

#The preProcess function returns a list of elements, such as vectors of standard deviations
# and vector of means since scale and center were reqeuested.
test_set <- preProcess(test.tune, method = c('scale', 'center'))

#The predict function uses the list of elements to produce a dataframe of pre-processed variables
transformed_test <- predict(test_set, test.tune)

```


Neural Network Regression Model

```{r}
y <- transformed_train$Yield
X <- subset(transformed_train, select = -c(Yield))

mod1 <- nnet(X, y, size = 10, linout = T, maxit = 500)

pred1 <- predict(mod1, transformed_test)
```

Support Vector Machines

```{r}
mod2 <- train(X, y, method = 'svmRadial', tuneLength = 14, trControl = trainControl(method = 'cv'))

mod2

pred2 <- predict(mod2$finalModel, transformed_test)
```
K-Nearest Neighbors

```{r}
mod3 <- train(X, y, method = 'knn', tuneGrid = data.frame(.k = 1:20),
              trControl = trainControl(method = 'cv'))

mod3

pred3 <- predict(mod3$finalModel, transformed_test)
```

Comparing RMSE for the three models:

```{r}
one <- RMSE(pred1, y_test)
print(paste0('Neural Network RMSE: ', one))
two <- RMSE(pred2, y_test)
print(paste0('Support Vector Machines RMSE: ', two))
three <- RMSE(pred3, y_test)
print(paste0('k-Nearest Neighbors RMSE: ', three))
```

The Support Vector Machines model has the lowest RMSE giving us the best model.

#### b) Which predictors are most important in the optimal nonlinear regression model? Do either biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

```{r}
varImp(mod2)
```

The top predictor variables from the SVM model are shown above.  From that list, compared with the top linear regression model variables, 'ManufacturingProcess32', 'BiologicalMaterial03', and 'BiologicalMaterial06' were the only variables that were in both lists. 

With the SVM model, there were 6 Manufacturing variables and 4 Biological variables in the top ten list.  Considering there are many more Manufacturing variables than Biological variables overall, a higher percentage of Biological variables that were included may mean more significance in that context. 

#### c) Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model.  Do these plots reveal intuition about the biological or process predictors and their relationship with yield?

Below, the actual data is in Black while the predicted data is in Red.  We can see that the accuracy is very good and doesn't seem to have much variance in most of the graph. 

```{r}
p <- data.frame('actual' = y_test, 'pred' = pred2)

matplot(p, type = 'l', pch = 1)
```

```{r}
#Calculate residuals
res <- y_test-pred2

plot(res)
abline(h=0, col='red')
```

The residuals for this model seem to have a uniform variance though the variance may get greater towards the end. There doesn't seem to be any noticeable skew. 

```{r}
top <- subset(test, select = c(Yield, ManufacturingProcess13, ManufacturingProcess17, BiologicalMaterial02,
                               BiologicalMaterial12, ManufacturingProcess36, ManufacturingProcess06,
                               ManufacturingProcess33, BiologicalMaterial11))

m <- cor(top)
corrplot(m, method = 'circle')
```

From these predictors we can see that many more variables are highly correlated. BioMaterial 11 and 12 are highly correlated as well as ManufProcess 13 and 17. An intuitive thought would be to look out for predictors that have negative correlation to Yield. ManufProcess13, 17, and 36 should be reformed or analyzed to understand why they are negatively correlated to Yield.







