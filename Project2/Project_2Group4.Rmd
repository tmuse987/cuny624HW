---
title: 'DATA 624 Spring 2019: Project-2'
author: "Ahmed Sajjad, Harpreet Shoker, Jagruti Solao, Chad Smith, Todd Weigel"
date: "April 29, 2019"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 3000)
```
```{r}
library(naniar)
library(corrplot)
library(caret)
library(corrr)
library(mice)
library(randomForest)
library(dplyr)
library(e1071)
library(kernlab)
library(ranger)
library(kableExtra)
```

Loading the data set 
```{r}
project_df <-read.csv("StudentData.csv",sep=",",header=TRUE,stringsAsFactors=FALSE)
dim(project_df)
```

From the above result dataset has 33 variables and 2571 observations.so we have 32 predictor variables and 1 target variable.
Lets look at summary of data

```{r}
summary(project_df)
#str(project_df)
```

From the summary results - The variable Brandcode is a character type .All the other variables are numeric and integers.
We can see null values in all variables except Air.Pressure and Pressure.Vacuum.We can have a look through plot

```{r}
project_df[, c(16, 18, 21, 28)] <- sapply(project_df[, c(16, 18, 21, 28)], as.numeric) # converting int to numeric
```

Lets look at the missing values in the dataset
```{r}
gg_miss_var(project_df)
```
The above plot shows that MFR has most missing values.

To have a better look to see distribution of the variables lets plot histogram of all variables

```{r}
par(mfrow = c(3,5), cex = .5)
project_df1 <- project_df[2:ncol(project_df)]
for(i in colnames(project_df1)){
hist(project_df1[,i], xlab = names(project_df1[i]),
  main = names(project_df1[i]), col="light blue", ylab="")
}
```
From the above some of the variables shows nearly normal distributions.Some of variables shows strong skewness that means the presence of outliers.
Also some variables have many near to zero values  Hyd.Pressure1, Hyd.Pressure2, and Hyd.Pressure3
Some varibles need transformations here.
(need to update all these)
Plotting boxplots of all variables will give better understanding with outliers.
```{r}
par(mfrow = c(3,5), cex = .5)
for(i in colnames(project_df1)){
boxplot(project_df1[,i], xlab = names(project_df1[i]),
  main = names(project_df1[i]), col="light blue", ylab="")
}
```

(update here with method to fix outliers)

CORRELATIONS BETWEEN VARIABLES :-

```{r}

plotcorr =cor(project_df1,use="pairwise.complete.obs", method = "pearson")

corrplot(plotcorr, method = "color",type = "upper", order = "original", number.cex = .7,tl.pos = "td",tl.cex = 0.5, tl.srt = 90,diag = TRUE)

```
```{r}

cor_var = findCorrelation(plotcorr, cutoff=0.75)
print(cor_var)
```

We have found out the variables that are very highly correlated with each other.

Carb.Volume with Density, Balling, Alch.Rel, Carb.Rel, and Balling.Lvl 

Carb.Pressure with Carb.Temp

Filler.Level with Bowl.Setpoint

Filler.Speed with MFR

Let us now look at the correlation between the target (pH) variable and the predictors.

```{r}
library(ggplot2)
x <- project_df1 %>% correlate() %>% focus(PH)
x %>% 
 mutate(rowname = factor(rowname, levels = rowname[order(PH)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = PH)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with PH") +
    xlab("Variables") + geom_col() + 
    coord_flip()

```


DATA PREprocessing 

Imputations - 
Variable Brand.code has a proportion of its data that is unspecified.
```{r}
library(plyr)
count(project_df$Brand.Code != "")
```
There are around 120 values that are unspecified we will replace blank codes with "N"

```{r}
project_df$Brand.Code[project_df$Brand.Code==""]<- "N"
count(project_df$Brand.Code != "")
```

For all other missing values we will be using MICE package and a Random Forest Regression.
```{r results='hide'}

project_df_new <- mice(data = project_df, m = 2, method = 'rf', maxit = 3)
```
Letâ€™s compare the distributions of original and imputed data using   plots.The density of the imputed data for each imputed dataset is showed in purple while the density of the observed data is showed in blue

```{r}
densityplot(project_df_new)
project_df_new1 <- complete(project_df_new)
```

NON Zero VARIANCE VARIABLES

```{r}
NZV = nearZeroVar(project_df_new1, saveMetrics = TRUE)
NZV[NZV[,"zeroVar"] > 0, ] 
NZV[NZV[,"zeroVar"] + NZV[,"nzv"] > 0, ]
```
We found that only Hyd Pressure 1 is the result.
Also we already found from hist plots we observed the high number of 0 values for variables; Hyd.Pressure1, Hyd.Pressure2, and Hyd.Pressure3. We can remove these 3 variables from dataset.

```{r}
project_df_new1 <- project_df_new1[, -c(13:15)]
```

```{r}
set.seed(143) 
sample = sample.int(n = nrow(project_df), size = floor(.70*nrow(project_df)), replace = F)

project_df_new1_train = project_df_new1[sample, ]
project_df_new1_test  = project_df_new1[-sample,]

```

## Model Creation
```{r}
myControl = trainControl(method = 'cv', number = 5, verboseIter = FALSE, savePredictions = TRUE, allowParallel = T)
```

### Linear Model
```{r eval = TRUE}
set.seed(143)
linearModel_data = train(PH ~ ., data = project_df_new1_train , metric = 'RMSE', method = 'lm', preProcess = c('center', 'scale'), trControl = myControl)
lm_pred = predict(linearModel_data, newdata = project_df_new1_test)
linearModel_data
```

### General Linear Model
```{r eval = TRUE}
set.seed(143)
generalLinearModel_data = train(PH ~ ., data = project_df_new1_train , metric = 'RMSE', method = 'glm', preProcess = c('center', 'scale'), trControl = myControl)
glm_pred = predict(generalLinearModel_data, newdata = project_df_new1_test)
generalLinearModel_data
```

### Support Vector Machines (SVM) Model
```{r eval = TRUE}
set.seed(143)
svmRadial_data = train(PH ~ ., data = project_df_new1_train, metric = 'RMSE', method = "svmRadial", preProc =c("center", "scale"), tuneLength = 14, trControl = myControl)
svm_pred = predict(svmRadial_data, newdata = project_df_new1_test)
svmRadial_data
```

### Random Forest Model
```{r eval = TRUE}
set.seed(143)
randomForest_data = train(PH ~ ., data = project_df_new1_train, metric = 'RMSE', method = 'ranger', preProcess = c('center', 'scale'), trControl = myControl)
rf_pred = predict(randomForest_data, newdata = project_df_new1_test)
randomForest_data
```

### Cubist Model

## Compare Models
```{r}
#Function to display model details like RMSE etc
showModelDetails <- function(model){ 
  rslts <- model$results %>%
    arrange(RMSE)
  head(rslts, 10) %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
}
showModelDetails(linearModel_data)
showModelDetails(generalLinearModel_data)
showModelDetails(svmRadial_data)
showModelDetails(randomForest_data)
```

<table>
</table>


## Conclusion

## References




