---
title: 'DATA 624 Spring 2019: Project-2'
author: "Ahmed Sajjad, Harpreet Shoker, Jagruti Solao, Chad Smith, Todd Weigel"
date: "April 29, 2019"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 3000)
```
```{r}
library(naniar)
library(corrplot)
library(caret)
library(corrr)
library(mice)
library(randomForest)
library(dplyr)
library(e1071)
library(kernlab)
library(ranger)
library(kableExtra)
library(ModelMetrics)
```

Loading the data set 
```{r}
project_df <-read.csv("StudentData.csv",sep=",",header=TRUE,stringsAsFactors=FALSE)
dim(project_df)
```

From the above result dataset has 33 variables and 2571 observations.so we have 32 predictor variables and 1 target variable.
Lets look at summary of data

```{r}
summary(project_df)
#str(project_df)
```

From the summary results - The variable Brandcode is a character type .All the other variables are numeric and integers.
We can see null values in all variables except Air.Pressure and Pressure.Vacuum.We can have a look through plot

```{r}
project_df[, c(16, 18, 21, 28)] <- sapply(project_df[, c(16, 18, 21, 28)], as.numeric) # converting int to numeric
```

Lets look at the missing values in the dataset
```{r}
gg_miss_var(project_df)
```
The above plot shows that MFR has most missing values.

To have a better look to see distribution of the variables lets plot histogram of all variables

```{r}
par(mfrow = c(3,5), cex = .5)
project_df1 <- project_df[2:ncol(project_df)]
for(i in colnames(project_df1)){
hist(project_df1[,i], xlab = names(project_df1[i]),
  main = names(project_df1[i]), col="light blue", ylab="")
}
```
From the above some of the variables shows nearly normal distributions.Some of variables shows strong skewness that means the presence of outliers.
Also some variables have many near to zero values  Hyd.Pressure1, Hyd.Pressure2, and Hyd.Pressure3
Some varibles need transformations here.
(need to update all these)
Plotting boxplots of all variables will give better understanding with outliers.
```{r}
par(mfrow = c(3,5), cex = .5)
for(i in colnames(project_df1)){
boxplot(project_df1[,i], xlab = names(project_df1[i]),
  main = names(project_df1[i]), col="light blue", ylab="")
}
```

(update here with method to fix outliers)

CORRELATIONS BETWEEN VARIABLES :-

```{r}

plotcorr =cor(project_df1,use="pairwise.complete.obs", method = "pearson")

corrplot(plotcorr, method = "color",type = "upper", order = "original", number.cex = .7,tl.pos = "td",tl.cex = 0.5, tl.srt = 90,diag = TRUE)

```
```{r}

cor_var = findCorrelation(plotcorr, cutoff=0.75)
print(cor_var)
```

We have found out the variables that are very highly correlated with each other.

Carb.Volume with Density, Balling, Alch.Rel, Carb.Rel, and Balling.Lvl 

Carb.Pressure with Carb.Temp

Filler.Level with Bowl.Setpoint

Filler.Speed with MFR

Let us now look at the correlation between the target (pH) variable and the predictors.

```{r}
library(ggplot2)
x <- project_df1 %>% correlate() %>% focus(PH)
x %>% 
 mutate(rowname = factor(rowname, levels = rowname[order(PH)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = PH)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with PH") +
    xlab("Variables") + geom_col() + 
    coord_flip()

```


DATA PREprocessing 

Imputations - 
Variable Brand.code has a proportion of its data that is unspecified.
```{r}
library(plyr)
count(project_df$Brand.Code != "")
```
There are around 120 values that are unspecified we will replace blank codes with "N"

```{r}
project_df$Brand.Code[project_df$Brand.Code==""]<- "N"
count(project_df$Brand.Code != "")
```

For all other missing values we will be using MICE package and a Random Forest Regression.
```{r results='hide'}

project_df_new <- mice(data = project_df, m = 2, method = 'rf', maxit = 3)
```
Letâ€™s compare the distributions of original and imputed data using   plots.The density of the imputed data for each imputed dataset is showed in purple while the density of the observed data is showed in blue

```{r}
densityplot(project_df_new)
project_df_new1 <- complete(project_df_new)
```

NON Zero VARIANCE VARIABLES

```{r}
NZV = nearZeroVar(project_df_new1, saveMetrics = TRUE)
NZV[NZV[,"zeroVar"] > 0, ] 
NZV[NZV[,"zeroVar"] + NZV[,"nzv"] > 0, ]
```
We found that only Hyd Pressure 1 is the result.
Also we already found from hist plots we observed the high number of 0 values for variables; Hyd.Pressure1, Hyd.Pressure2, and Hyd.Pressure3. We can remove these 3 variables from dataset.

```{r}
project_df_new1 <- project_df_new1[, -c(13:15)]
```

```{r}
set.seed(143) 
sample = sample.int(n = nrow(project_df), size = floor(.70*nrow(project_df)), replace = F)

project_df_new1_train = project_df_new1[sample, ]
project_df_new1_test  = project_df_new1[-sample,]

```

## Model Creation
```{r}
myControl = trainControl(method = 'cv', number = 5, verboseIter = FALSE, savePredictions = TRUE, allowParallel = T)
```

### Linear Model
```{r eval = TRUE}
set.seed(143)
linearModel_data = train(PH ~ ., data = project_df_new1_train , metric = 'RMSE', method = 'lm', preProcess = c('center', 'scale'), trControl = myControl)
lm_pred = predict(linearModel_data, newdata = project_df_new1_test)
linearModel_data
```

### Support Vector Machines (SVM) Model
```{r eval = TRUE}
set.seed(143)
svmRadial_data = train(PH ~ ., data = project_df_new1_train, metric = 'RMSE', method = "svmRadial", preProc =c("center", "scale"), tuneLength = 14, trControl = myControl)
svm_pred = predict(svmRadial_data, newdata = project_df_new1_test)
svmRadial_data
```

### Random Forest Model
```{r eval = TRUE}
set.seed(143)
randomForest_data = train(PH ~ ., data = project_df_new1_train, metric = 'RMSE', method = 'ranger', preProcess = c('center', 'scale'), trControl = myControl)
rf_pred = predict(randomForest_data, newdata = project_df_new1_test)
randomForest_data
```

### Cubist Model

####Reread all data to start fresh.
```{r}
project_df <-read.csv("StudentData.csv",sep=",",header=TRUE,stringsAsFactors=FALSE)

#cleanup
project_df$Brand.Code[project_df$Brand.Code==""]<- "N"

#compute imputed values for missing values
project_df_new <- mice(data = project_df, m = 2, method = 'rf', maxit = 3)
#and  recreate the dataframe
project_df <- complete(project_df_new)
```

###Create variables to hold data in future steps.
```{r}

PHTest <- NULL
dfTest <- NULL
PHTrain <- NULL
dfTrain <- NULL
PH <- project_df$PH
project_df <<- project_df[, !(names(project_df) %in% c("PH"))]
```

####Add "dummy"" variables for the categorical column brand.

```{r}

#create dummy variables for the categorical brand.code
dummys <- dummyVars(~Brand.Code, data=project_df)
dfDummys <- data.frame(predict(dummys, project_df))
project_df <- bind_cols(project_df, dfDummys)
project_df <- project_df[,-1]

```

Set a seed for repeatability.  Then create a function to partition data in to test and train sets.  80% goes to train, 20% to test.

```{r}
seed <- 42
set.seed(seed)
partition <- function(df)
{
    #divide into test and train
 
    partitions <- createDataPartition(seq(1, nrow(df)), p = 0.8, list = FALSE)
    dfTrain <<- df[partitions,]
    dfTest <<- df[-partitions,]
    
    PHTrain <<- PH[partitions]
    PHTest <<- PH[-partitions]
}

```
###Preprocess the data.  
We will globably center, scale and perform  boxcox transform the data.  Note each of these was tested individually, and performance improved with each of these actions.  PCA was tried, but even removing any of the low performing predictors had a negative impact of RMSE.  If we want to improve performance we could do PCA at 95% and remove about a third of the variables.

We then call the partition function, to create the dftrain, dftest and the PHTrain and PHTest datasets.

From results of the preprocessing, we see all values were scaled and centered, and 22 had a boxcox transformation applied.
```{r}
##doing PCA at 95% level increases RMSE noticably (10% or so), so we won't do it.
df <- project_df
pp <- preProcess(df, method = c("BoxCox", "center", "scale"))
pp
df <- predict(pp, df)
df <- partition(df)

```
###CV object

Set up a control object that will do cross validation, so that we improve results, by reducing chance of poor randomness in any inital paritioning.

```{r}

#use cross validation
#the more folds, the better the results, but the longer the processing takes.
controlObject <- trainControl(method = "repeatedcv", repeats = 5, number = 10)

```

###Cubist Grid.  
The best performance from a number of different models were compared.  Cubist grid was the best of those testsed.  Neural Networks were a close second, but took forever (hours) to get close.  SVM was also close in performance.

First set up the grid of test data.  
"Committees" are the sets of models tested and chained together.  Prior models affect future models, as each data point's weight in the current model is adjusted based on it performance in the prior model.
Neighbors is a KNN algorithm that can further adjust the model based on each datapoints neighbors.

The print of the cubistgrid shows all the variations that will be tested.

Note, while developing, no test with small committees was chosen as the best model, so we are only searching with committee size of 50 or above.

```{r}
cubistGrid <- expand.grid(.committees = seq(50,100,10),
                          .neighbors = seq(1,9,2))
cubistGrid
```
###Execute the Cubist Model.  
Note commented out because it takes forever.  We will load a saved version in next cell, that has the computed model already created.
```{r}
# cubistModel <- train(dfTrain, PHTrain,
#                      method = "cubist",
#                      tuneGrid = cubistGrid,
#                      trControl = controlObject)
# saveRDS(cubistModel, file = "cubistModel.rds")
```



###Load the saved cubist model.  And show results

We see that we get over 80% correlation on the test PH predicted value to the actual test value.  And RMSE is about .10
```{R}
cubistModel <- readRDS("cubistModel.rds")
print(cubistModel)
cubistPred <- predict(cubistModel, dfTest)
xyplot(PHTest ~ cubistPred)
print(cor(PHTest, cubistPred))
print(rmse(PHTest, cubistPred))

```
## Compare Models
```{r}
#Function to display model details like RMSE etc
showModelDetails <- function(model){ 
  rslts <- model$results %>%
    arrange(RMSE)
  head(rslts, 10) %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
}
showModelDetails(linearModel_data)
showModelDetails(svmRadial_data)
showModelDetails(randomForest_data)
showModelDetails(cubistModel)
```

<table>
</table>


##Load "Student Evaluation to Predict" dataset.  

```{r}

dfToPredict <-read.csv("StudentEvaluation- TO PREDICT.csv",sep=",",header=TRUE,stringsAsFactors=FALSE)
df <- dfToPredict
```

#Clean up dataset 
Just as we did with the training test file.

```{r}

###commented out, we will just load "cleaned up file"
#cleanup

#Remove PH Column
df <- df[, !(names(df) %in% c("PH"))]

df$Brand.Code[df$Brand.Code==""]<- "N"

#compute imputed values for missing values
df <- mice(data = df, m = 2, method = 'rf', maxit = 3)
#and  recreate the dataframe
df <- complete(df)
#write.csv(df, file = "StudentEvaluation- TO PREDICT_Clean.csv", row.names = FALSE)
# # 
#df <-read.csv("StudentEvaluation- TO PREDICT_Clean.csv",sep=",",header=TRUE,stringsAsFactors=FALSE)



```
###Again Create Dummy VARS

```{r}
#create dummy variables for the categorical brand.code

dummys <- dummyVars(~Brand.Code, data=df)
dfDummys <- data.frame(predict(dummys, df))
df <- bind_cols(df, dfDummys)
df <- df[,-1]

```
###Again Preprocess the data
(But we don't need to partition)

```{r}
df <- df
pp <- preProcess(df, method = c("BoxCox", "center", "scale"))
pp
df <- predict(pp, df)

```
Do our prediction, put them in the predicted dataset and save to a file
```{r}
cubistPred <- predict(cubistModel, df)
#cubistPred
dfToPredict$PH <- cubistPred
write.csv(dfToPredict, "StudentDateWithPredictedPH.csv", row.names = FALSE)
```

## Conclusion

## References




